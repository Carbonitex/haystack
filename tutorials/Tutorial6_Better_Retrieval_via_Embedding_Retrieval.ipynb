{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEH-CRbeA6NU"
      },
      "source": [
        "# Better Retrieval via \"Embedding Retrieval\"\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial6_Better_Retrieval_via_Embedding_Retrieval.ipynb)\n",
        "\n",
        "### Importance of Retrievers\n",
        "\n",
        "The Retriever has a huge impact on the performance of our overall search pipeline.\n",
        "\n",
        "\n",
        "### Different types of Retrievers\n",
        "#### Sparse\n",
        "Family of algorithms based on counting the occurrences of words (bag-of-words) resulting in very sparse vectors with length = vocab size.\n",
        "\n",
        "**Examples**: BM25, TF-IDF\n",
        "\n",
        "**Pros**: Simple, fast, well explainable\n",
        "\n",
        "**Cons**: Relies on exact keyword matches between query and text\n",
        " \n",
        "\n",
        "#### Dense\n",
        "These retrievers use neural network models to create \"dense\" embedding vectors. Within this family, there are two different approaches:\n",
        "\n",
        "a) Single encoder: Use a **single model** to embed both the query and the passage.\n",
        "b) Dual-encoder: Use **two models**, one to embed the query and one to embed the passage.\n",
        "\n",
        "**Examples**: REALM, DPR, Sentence-Transformers\n",
        "\n",
        "**Pros**: Captures semantic similarity instead of \"word matches\" (for example, synonyms, related topics).\n",
        "\n",
        "**Cons**: Computationally more heavy to use, initial training of the model (though this is less of an issue nowadays as many pre-trained models are available and most of the time, it's not needed to train the model).\n",
        "\n",
        "\n",
        "### Embedding Retrieval\n",
        "\n",
        "In this Tutorial, we use an `EmbeddingRetriever` with [Sentence Transformers](https://www.sbert.net/index.html) models.\n",
        "\n",
        "These models are trained to embed similar sentences close to each other in a shared embedding space.\n",
        "\n",
        "Some models have been fine-tuned on massive Information Retrieval data and can be used to retrieve documents based on a short query (for example, `multi-qa-mpnet-base-dot-v1`). There are others that are more suited to semantic similarity tasks where you are trying to find the most similar documents to a given document (for example, `all-mpnet-base-v2`). There are even models that are multilingual (for example, `paraphrase-multilingual-mpnet-base-v2`). For a good overview of different models with their evaluation metrics, see the [Pretrained Models](https://www.sbert.net/docs/pretrained_models.html#) in the Sentence Transformers documentation.\n",
        "\n",
        "*Use this* [link](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial6_Better_Retrieval_via_Embedding_Retrieval.ipynb) *to open the notebook in Google Colab.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K27Y5FbA6NV"
      },
      "source": [
        "### Prepare the Environment\n",
        "\n",
        "#### Colab: Enable the GPU Runtime\n",
        "Make sure you enable the GPU runtime to experience decent speed in this tutorial.\n",
        "**Runtime -> Change Runtime type -> Hardware accelerator -> GPU**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/colab_gpu_runtime.jpg\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlZgP8q1A6NW"
      },
      "outputs": [],
      "source": [
        "# Make sure you have a GPU running\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM36kbRFA6Nc"
      },
      "outputs": [],
      "source": [
        "# Install the latest release of Haystack in your own environment\n",
        "#! pip install farm-haystack\n",
        "\n",
        "# Install the latest main of Haystack\n",
        "!pip install --upgrade pip\n",
        "!pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab,faiss]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "GbM2ml-ozqLX",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Logging\n",
        "\n",
        "We configure how logging messages should be displayed and which log level should be used before importing Haystack.\n",
        "Example log message:\n",
        "INFO - haystack.utils.preprocessing -  Converting data/tutorial1/218_Olenna_Tyrell.txt\n",
        "Default log level in basicConfig is WARNING so the explicit parameter is not necessary but can be changed easily:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kQWEUUMnzqLX",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
        "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xmRuhTQ7A6Nh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tstad/miniconda3/envs/haystack-dev/lib/python3.7/site-packages/mlflow/types/schema.py:48: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  binary = (7, np.dtype(\"bytes\"), \"BinaryType\", np.object)\n",
            "INFO - haystack.modeling.model.optimization -  apex is available.\n",
            "INFO - haystack.modeling.model.optimization -  apex.parallel is available.\n",
            "/home/tstad/miniconda3/envs/haystack-dev/lib/python3.7/site-packages/espnet2/gan_tts/vits/vits.py:43: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if LooseVersion(torch.__version__) >= LooseVersion(\"1.6.0\"):\n"
          ]
        }
      ],
      "source": [
        "from haystack.utils import clean_wiki_text, convert_files_to_docs, fetch_archive_from_http, print_answers\n",
        "from haystack.nodes import FARMReader, TransformersReader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3dSo7ZtA6Nl"
      },
      "source": [
        "### Document Store\n",
        "\n",
        "#### Option 1: FAISS\n",
        "\n",
        "FAISS is a library for efficient similarity search on a cluster of dense vectors.\n",
        "The `FAISSDocumentStore` uses a SQL(SQLite in-memory be default) database under-the-hood\n",
        "to store the document text and other meta data. The vector embeddings of the text are\n",
        "indexed on a FAISS Index that later is queried for searching answers.\n",
        "The default flavour of FAISSDocumentStore is \"Flat\" but can also be set to \"HNSW\" for\n",
        "faster search at the expense of some accuracy. Just set the faiss_index_factor_str argument in the constructor.\n",
        "For more info on which suits your use case: https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1cYgDJmrA6Nv",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from haystack.document_stores.plaid import PlaidDocumentStore\n",
        "\n",
        "document_store = PlaidDocumentStore(plaid_index_path=\"/home/tstad/git/haystack/data/plaid_index\", use_gpu=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "s4HK5l0qzqLZ",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### Option 2: Milvus\n",
        "\n",
        "Milvus is an open source database library that is also optimized for vector similarity searches like FAISS.\n",
        "Like FAISS it has both a \"Flat\" and \"HNSW\" mode but it outperforms FAISS when it comes to dynamic data management.\n",
        "It does require a little more setup, however, as it is run through Docker and requires the setup of some config files.\n",
        "See [their docs](https://milvus.io/docs/v1.0.0/milvus_docker-cpu.md) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2Ur4h-E3zqLZ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Milvus cannot be run on COlab, so this cell is commented out.\n",
        "# To run Milvus you need Docker (versions below 2.0.0) or a docker-compose (versions >= 2.0.0), neither of which is available on Colab.\n",
        "# See Milvus' documentation for more details: https://milvus.io/docs/install_standalone-docker.md\n",
        "\n",
        "# !pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[milvus]\n",
        "\n",
        "# from haystack.utils import launch_milvus\n",
        "# from haystack.document_stores import MilvusDocumentStore\n",
        "\n",
        "# launch_milvus()\n",
        "# document_store = MilvusDocumentStore()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06LatTJBA6N0",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Cleaning & indexing documents\n",
        "\n",
        "Similarly to the previous tutorials, we download, convert and index some Game of Thrones articles to our DocumentStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqKnu6wxA6N1",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Let's first get some files that we want to use\n",
        "doc_dir = \"data/tutorial6\"\n",
        "s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt6.zip\"\n",
        "fetch_archive_from_http(url=s3_url, output_dir=doc_dir)\n",
        "\n",
        "# Convert files to dicts\n",
        "docs = convert_files_to_docs(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=True)\n",
        "\n",
        "# Now, let's write the dicts containing documents to our DB.\n",
        "document_store.write_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgjedxx_A6N6"
      },
      "source": [
        "### Initialize Retriever, Reader & Pipeline\n",
        "\n",
        "#### Retriever\n",
        "\n",
        "**Here:** We use an `EmbeddingRetriever`.\n",
        "\n",
        "**Alternatives:**\n",
        "\n",
        "- `BM25Retriever` with custom queries (for example, boosting) and filters\n",
        "- `DensePassageRetriever` which uses two encoder models, one to embed the query and one to embed the passage, and then compares the embedding for retrieval\n",
        "- `TfidfRetriever` in combination with a SQL or InMemory Document store for simple prototyping and debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kFwiPP60A6N7",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO - haystack.modeling.utils -  Using devices: CUDA:0\n",
            "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
            "INFO - haystack.nodes.retriever.dense -  Init retriever using embeddings of model /home/tstad/git/ColBERT/docs/downloads/colbertv2.0\n"
          ]
        }
      ],
      "source": [
        "from haystack.nodes import EmbeddingRetriever\n",
        "\n",
        "retriever = EmbeddingRetriever(\n",
        "    document_store=document_store,\n",
        "    embedding_model=\"/home/tstad/git/ColBERT/docs/downloads/colbertv2.0\",\n",
        "    model_format=\"colbert\",\n",
        ")\n",
        "# Important:\n",
        "# Now that we initialized the Retriever, we need to call update_embeddings() to iterate over all\n",
        "# previously indexed documents and update their embedding representation.\n",
        "# While this can be a time consuming operation (depending on the corpus size), it only needs to be done once.\n",
        "# At query time, we only need to embed the query and compare it to the existing document embeddings, which is very fast.\n",
        "# document_store.update_embeddings(retriever)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnVR28OXA6OA"
      },
      "source": [
        "#### Reader\n",
        "\n",
        "Similar to previous Tutorials we now initalize our reader.\n",
        "\n",
        "Here we use a FARMReader with the *deepset/roberta-base-squad2* model (see: https://huggingface.co/deepset/roberta-base-squad2)\n",
        "\n",
        "\n",
        "\n",
        "##### FARMReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyIuWVwhA6OB"
      },
      "outputs": [],
      "source": [
        "# Load a  local model or any of the QA models on\n",
        "# Hugging Face's model hub (https://huggingface.co/models)\n",
        "\n",
        "# reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unhLD18yA6OF"
      },
      "source": [
        "### Pipeline\n",
        "\n",
        "With a Haystack `Pipeline` you can stick together your building blocks to a search pipeline.\n",
        "Under the hood, `Pipelines` are Directed Acyclic Graphs (DAGs) that you can easily customize for your own use cases.\n",
        "To speed things up, Haystack also comes with a few predefined Pipelines. One of them is the `ExtractiveQAPipeline` that combines a retriever and a reader to answer our questions.\n",
        "You can learn more about `Pipelines` in the [docs](https://haystack.deepset.ai/docs/latest/pipelinesmd)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TssPQyzWA6OG"
      },
      "outputs": [],
      "source": [
        "from haystack.pipelines import DocumentSearchPipeline\n",
        "\n",
        "pipe = DocumentSearchPipeline(retriever)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXlBBxKXA6OL"
      },
      "source": [
        "## Voilà! Ask a question!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zi97Hif2A6OM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Sep 16, 20:37:26] #> Loading codec...\n",
            "[Sep 16, 20:37:26] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Sep 16, 20:37:26] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Sep 16, 20:37:27] #> Loading IVF...\n",
            "[Sep 16, 20:37:27] #> Loading doclens...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1345.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Sep 16, 20:37:27] #> Loading codes and residuals...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 160.72it/s]\n"
          ]
        },
        {
          "ename": "Exception",
          "evalue": "Exception while running node `Retriever` with input `{'root_node': 'Query', 'params': {'Retriever': {'top_k': 10}}, 'query': 'Who created the Dothraki vocabulary?', 'node_id': 'Retriever'}`: '1021' is not in list, full stack trace: Traceback (most recent call last):\n  File \"/home/tstad/git/haystack/haystack/pipelines/base.py\", line 512, in run\n    node_output, stream_id = self._run_node(node_id, node_input)\n  File \"/home/tstad/git/haystack/haystack/pipelines/base.py\", line 439, in _run_node\n    return self.graph.nodes[node_id][\"component\"]._dispatch_run(**node_input)\n  File \"/home/tstad/git/haystack/haystack/nodes/base.py\", line 201, in _dispatch_run\n    return self._dispatch_run_general(self.run, **kwargs)\n  File \"/home/tstad/git/haystack/haystack/nodes/base.py\", line 245, in _dispatch_run_general\n    output, stream = run_method(**run_inputs, **run_params)\n  File \"/home/tstad/git/haystack/haystack/nodes/retriever/base.py\", line 279, in run\n    query=query, filters=filters, top_k=top_k, index=index, headers=headers, scale_score=scale_score\n  File \"/home/tstad/git/haystack/haystack/nodes/retriever/base.py\", line 109, in wrapper\n    ret = fn(*args, **kwargs)\n  File \"/home/tstad/git/haystack/haystack/nodes/retriever/base.py\", line 328, in run_query\n    query=query, filters=filters, top_k=top_k, index=index, headers=headers, scale_score=scale_score\n  File \"/home/tstad/git/haystack/haystack/nodes/retriever/dense.py\", line 1644, in retrieve\n    query_emb=query_emb[0], filters=filters, top_k=top_k, index=index, headers=headers, scale_score=scale_score\n  File \"/home/tstad/git/haystack/haystack/document_stores/plaid.py\", line 654, in query_by_embedding\n    documents = self.get_documents_by_vector_ids(pids, index=index)\n  File \"/home/tstad/git/haystack/haystack/document_stores/sql.py\", line 217, in get_documents_by_vector_ids\n    sorted_documents = sorted(documents, key=lambda doc: vector_ids.index(doc.meta[\"vector_id\"]))\n  File \"/home/tstad/git/haystack/haystack/document_stores/sql.py\", line 217, in <lambda>\n    sorted_documents = sorted(documents, key=lambda doc: vector_ids.index(doc.meta[\"vector_id\"]))\nValueError: '1021' is not in list\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m~/git/haystack/haystack/pipelines/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, query, file_paths, labels, documents, meta, params, debug)\u001b[0m\n\u001b[1;32m    511\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Running node `{node_id}` with input `{node_input}`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                     \u001b[0mnode_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/git/haystack/haystack/pipelines/base.py\u001b[0m in \u001b[0;36m_run_node\u001b[0;34m(self, node_id, node_input)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"component\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnode_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/git/haystack/haystack/nodes/base.py\u001b[0m in \u001b[0;36m_dispatch_run\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \"\"\"\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_run_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/git/haystack/haystack/nodes/base.py\u001b[0m in \u001b[0;36m_dispatch_run_general\u001b[0;34m(self, run_method, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrun_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrun_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/git/haystack/haystack/nodes/retriever/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, root_node, query, filters, top_k, documents, index, headers, scale_score)\u001b[0m\n\u001b[1;32m    278\u001b[0m             output, stream = run_query_timed(\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m             )\n",
            "\u001b[0;32m~/git/haystack/haystack/nodes/retriever/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mtoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/git/haystack/haystack/nodes/retriever/base.py\u001b[0m in \u001b[0;36mrun_query\u001b[0;34m(self, query, filters, top_k, index, headers, scale_score)\u001b[0m\n\u001b[1;32m    327\u001b[0m         documents = self.retrieve(\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         )\n",
            "\u001b[0;32m~/git/haystack/haystack/nodes/retriever/dense.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self, query, filters, top_k, index, headers, scale_score)\u001b[0m\n\u001b[1;32m   1643\u001b[0m         documents = self.document_store.query_by_embedding(\n\u001b[0;32m-> 1644\u001b[0;31m             \u001b[0mquery_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1645\u001b[0m         )\n",
            "\u001b[0;32m~/git/haystack/haystack/document_stores/plaid.py\u001b[0m in \u001b[0;36mquery_by_embedding\u001b[0;34m(self, query_emb, filters, top_k, index, return_embedding, headers, scale_score)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_documents_by_vector_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/git/haystack/haystack/document_stores/sql.py\u001b[0m in \u001b[0;36mget_documents_by_vector_ids\u001b[0;34m(self, vector_ids, index, batch_size)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0msorted_documents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvector_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vector_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msorted_documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/git/haystack/haystack/document_stores/sql.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0msorted_documents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvector_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vector_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msorted_documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: '1021' is not in list",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_3304310/2748250841.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# The higher top_k for retriever, the better (but also the slower) your answers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m prediction = pipe.run(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Who created the Dothraki vocabulary?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Retriever\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m~/git/haystack/haystack/pipelines/standard_pipelines.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, query, params, debug)\u001b[0m\n\u001b[1;32m    411\u001b[0m               \u001b[0mby\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0munder\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m\"_debug\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \"\"\"\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/git/haystack/haystack/pipelines/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, query, file_paths, labels, documents, meta, params, debug)\u001b[0m\n\u001b[1;32m    514\u001b[0m                     \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m                     raise Exception(\n\u001b[0;32m--> 516\u001b[0;31m                         \u001b[0;34mf\"Exception while running node `{node_id}` with input `{node_input}`: {e}, full stack trace: {tb}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m                     )\n\u001b[1;32m    518\u001b[0m                 \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Exception while running node `Retriever` with input `{'root_node': 'Query', 'params': {'Retriever': {'top_k': 10}}, 'query': 'Who created the Dothraki vocabulary?', 'node_id': 'Retriever'}`: '1021' is not in list, full stack trace: Traceback (most recent call last):\n  File \"/home/tstad/git/haystack/haystack/pipelines/base.py\", line 512, in run\n    node_output, stream_id = self._run_node(node_id, node_input)\n  File \"/home/tstad/git/haystack/haystack/pipelines/base.py\", line 439, in _run_node\n    return self.graph.nodes[node_id][\"component\"]._dispatch_run(**node_input)\n  File \"/home/tstad/git/haystack/haystack/nodes/base.py\", line 201, in _dispatch_run\n    return self._dispatch_run_general(self.run, **kwargs)\n  File \"/home/tstad/git/haystack/haystack/nodes/base.py\", line 245, in _dispatch_run_general\n    output, stream = run_method(**run_inputs, **run_params)\n  File \"/home/tstad/git/haystack/haystack/nodes/retriever/base.py\", line 279, in run\n    query=query, filters=filters, top_k=top_k, index=index, headers=headers, scale_score=scale_score\n  File \"/home/tstad/git/haystack/haystack/nodes/retriever/base.py\", line 109, in wrapper\n    ret = fn(*args, **kwargs)\n  File \"/home/tstad/git/haystack/haystack/nodes/retriever/base.py\", line 328, in run_query\n    query=query, filters=filters, top_k=top_k, index=index, headers=headers, scale_score=scale_score\n  File \"/home/tstad/git/haystack/haystack/nodes/retriever/dense.py\", line 1644, in retrieve\n    query_emb=query_emb[0], filters=filters, top_k=top_k, index=index, headers=headers, scale_score=scale_score\n  File \"/home/tstad/git/haystack/haystack/document_stores/plaid.py\", line 654, in query_by_embedding\n    documents = self.get_documents_by_vector_ids(pids, index=index)\n  File \"/home/tstad/git/haystack/haystack/document_stores/sql.py\", line 217, in get_documents_by_vector_ids\n    sorted_documents = sorted(documents, key=lambda doc: vector_ids.index(doc.meta[\"vector_id\"]))\n  File \"/home/tstad/git/haystack/haystack/document_stores/sql.py\", line 217, in <lambda>\n    sorted_documents = sorted(documents, key=lambda doc: vector_ids.index(doc.meta[\"vector_id\"]))\nValueError: '1021' is not in list\n"
          ]
        }
      ],
      "source": [
        "# You can configure how many candidates the reader and retriever shall return\n",
        "# The higher top_k for retriever, the better (but also the slower) your answers.\n",
        "prediction = pipe.run(\n",
        "    query=\"Who created the Dothraki vocabulary?\", params={\"Retriever\": {\"top_k\": 10}}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pI0wrHylzqLa"
      },
      "outputs": [],
      "source": [
        "print_answers(prediction, details=\"minimum\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "kXE84-2_zqLa"
      },
      "source": [
        "## About us\n",
        "\n",
        "This [Haystack](https://github.com/deepset-ai/haystack/) notebook was made with love by [deepset](https://deepset.ai/) in Berlin, Germany\n",
        "\n",
        "We bring NLP to the industry via open source!\n",
        "  \n",
        "Our focus: Industry specific language models & large scale QA systems.  \n",
        "  \n",
        "Some of our other work: \n",
        "- [German BERT](https://deepset.ai/german-bert)\n",
        "- [GermanQuAD and GermanDPR](https://deepset.ai/germanquad)\n",
        "- [FARM](https://github.com/deepset-ai/FARM)\n",
        "\n",
        "Get in touch:\n",
        "[Twitter](https://twitter.com/deepset_ai) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Slack](https://haystack.deepset.ai/community/join) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://deepset.ai)\n",
        "\n",
        "By the way: [we're hiring!](https://www.deepset.ai/jobs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Tutorial6_Better_Retrieval_via_Embedding_Retrieval.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.11 ('haystack-dev')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "a1c4180befe5334d9af26d84758dc08f43161c3b98a4eb4d4a43d7491d015a65"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
