{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEH-CRbeA6NU"
   },
   "source": [
    "# Better Retrieval via \"Embedding Retrieval\"\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial6_Better_Retrieval_via_Embedding_Retrieval.ipynb)\n",
    "\n",
    "### Importance of Retrievers\n",
    "\n",
    "The Retriever has a huge impact on the performance of our overall search pipeline.\n",
    "\n",
    "\n",
    "### Different types of Retrievers\n",
    "#### Sparse\n",
    "Family of algorithms based on counting the occurrences of words (bag-of-words) resulting in very sparse vectors with length = vocab size.\n",
    "\n",
    "**Examples**: BM25, TF-IDF\n",
    "\n",
    "**Pros**: Simple, fast, well explainable\n",
    "\n",
    "**Cons**: Relies on exact keyword matches between query and text\n",
    " \n",
    "\n",
    "#### Dense\n",
    "These retrievers use neural network models to create \"dense\" embedding vectors. Within this family, there are two different approaches:\n",
    "\n",
    "a) Single encoder: Use a **single model** to embed both the query and the passage.\n",
    "b) Dual-encoder: Use **two models**, one to embed the query and one to embed the passage.\n",
    "\n",
    "**Examples**: REALM, DPR, Sentence-Transformers\n",
    "\n",
    "**Pros**: Captures semantic similarity instead of \"word matches\" (for example, synonyms, related topics).\n",
    "\n",
    "**Cons**: Computationally more heavy to use, initial training of the model (though this is less of an issue nowadays as many pre-trained models are available and most of the time, it's not needed to train the model).\n",
    "\n",
    "\n",
    "### Embedding Retrieval\n",
    "\n",
    "In this Tutorial, we use an `EmbeddingRetriever` with [Sentence Transformers](https://www.sbert.net/index.html) models.\n",
    "\n",
    "These models are trained to embed similar sentences close to each other in a shared embedding space.\n",
    "\n",
    "Some models have been fine-tuned on massive Information Retrieval data and can be used to retrieve documents based on a short query (for example, `multi-qa-mpnet-base-dot-v1`). There are others that are more suited to semantic similarity tasks where you are trying to find the most similar documents to a given document (for example, `all-mpnet-base-v2`). There are even models that are multilingual (for example, `paraphrase-multilingual-mpnet-base-v2`). For a good overview of different models with their evaluation metrics, see the [Pretrained Models](https://www.sbert.net/docs/pretrained_models.html#) in the Sentence Transformers documentation.\n",
    "\n",
    "*Use this* [link](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial6_Better_Retrieval_via_Embedding_Retrieval.ipynb) *to open the notebook in Google Colab.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3K27Y5FbA6NV"
   },
   "source": [
    "### Prepare the Environment\n",
    "\n",
    "#### Colab: Enable the GPU Runtime\n",
    "Make sure you enable the GPU runtime to experience decent speed in this tutorial.\n",
    "**Runtime -> Change Runtime type -> Hardware accelerator -> GPU**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/colab_gpu_runtime.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlZgP8q1A6NW"
   },
   "outputs": [],
   "source": [
    "# Make sure you have a GPU running\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NM36kbRFA6Nc"
   },
   "outputs": [],
   "source": [
    "# # Install the latest release of Haystack in your own environment\n",
    "# #! pip install farm-haystack\n",
    "\n",
    "# # Install the latest main of Haystack\n",
    "# !pip install --upgrade pip\n",
    "# !pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab,faiss]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "GbM2ml-ozqLX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Logging\n",
    "\n",
    "We configure how logging messages should be displayed and which log level should be used before importing Haystack.\n",
    "Example log message:\n",
    "INFO - haystack.utils.preprocessing -  Converting data/tutorial1/218_Olenna_Tyrell.txt\n",
    "Default log level in basicConfig is WARNING so the explicit parameter is not necessary but can be changed easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kQWEUUMnzqLX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xmRuhTQ7A6Nh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tstad/miniconda3/envs/haystack-dev/lib/python3.7/site-packages/mlflow/types/schema.py:48: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  binary = (7, np.dtype(\"bytes\"), \"BinaryType\", np.object)\n",
      "INFO - haystack.modeling.model.optimization -  apex is available.\n",
      "INFO - haystack.modeling.model.optimization -  apex.parallel is available.\n",
      "/home/tstad/miniconda3/envs/haystack-dev/lib/python3.7/site-packages/espnet2/gan_tts/vits/vits.py:43: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(torch.__version__) >= LooseVersion(\"1.6.0\"):\n"
     ]
    }
   ],
   "source": [
    "from haystack.utils import clean_wiki_text, convert_files_to_docs, fetch_archive_from_http, print_answers\n",
    "from haystack.nodes import FARMReader, TransformersReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3dSo7ZtA6Nl"
   },
   "source": [
    "### Document Store\n",
    "\n",
    "#### Option 1: FAISS\n",
    "\n",
    "FAISS is a library for efficient similarity search on a cluster of dense vectors.\n",
    "The `FAISSDocumentStore` uses a SQL(SQLite in-memory be default) database under-the-hood\n",
    "to store the document text and other meta data. The vector embeddings of the text are\n",
    "indexed on a FAISS Index that later is queried for searching answers.\n",
    "The default flavour of FAISSDocumentStore is \"Flat\" but can also be set to \"HNSW\" for\n",
    "faster search at the expense of some accuracy. Just set the faiss_index_factor_str argument in the constructor.\n",
    "For more info on which suits your use case: https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1cYgDJmrA6Nv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from haystack.document_stores.plaid import PlaidDocumentStore\n",
    "\n",
    "document_store = PlaidDocumentStore(plaid_index_path=\"/home/tstad/git/haystack/data/plaid_index\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "s4HK5l0qzqLZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Option 2: Milvus\n",
    "\n",
    "Milvus is an open source database library that is also optimized for vector similarity searches like FAISS.\n",
    "Like FAISS it has both a \"Flat\" and \"HNSW\" mode but it outperforms FAISS when it comes to dynamic data management.\n",
    "It does require a little more setup, however, as it is run through Docker and requires the setup of some config files.\n",
    "See [their docs](https://milvus.io/docs/v1.0.0/milvus_docker-cpu.md) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2Ur4h-E3zqLZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Milvus cannot be run on COlab, so this cell is commented out.\n",
    "# To run Milvus you need Docker (versions below 2.0.0) or a docker-compose (versions >= 2.0.0), neither of which is available on Colab.\n",
    "# See Milvus' documentation for more details: https://milvus.io/docs/install_standalone-docker.md\n",
    "\n",
    "# !pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[milvus]\n",
    "\n",
    "# from haystack.utils import launch_milvus\n",
    "# from haystack.document_stores import MilvusDocumentStore\n",
    "\n",
    "# launch_milvus()\n",
    "# document_store = MilvusDocumentStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06LatTJBA6N0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cleaning & indexing documents\n",
    "\n",
    "Similarly to the previous tutorials, we download, convert and index some Game of Thrones articles to our DocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iqKnu6wxA6N1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Let's first get some files that we want to use\n",
    "# doc_dir = \"data/tutorial6\"\n",
    "# s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt6.zip\"\n",
    "# fetch_archive_from_http(url=s3_url, output_dir=doc_dir)\n",
    "\n",
    "# # Convert files to dicts\n",
    "# docs = convert_files_to_docs(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=True)\n",
    "\n",
    "# # Now, let's write the dicts containing documents to our DB.\n",
    "# document_store.write_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgjedxx_A6N6"
   },
   "source": [
    "### Initialize Retriever, Reader & Pipeline\n",
    "\n",
    "#### Retriever\n",
    "\n",
    "**Here:** We use an `EmbeddingRetriever`.\n",
    "\n",
    "**Alternatives:**\n",
    "\n",
    "- `BM25Retriever` with custom queries (for example, boosting) and filters\n",
    "- `DensePassageRetriever` which uses two encoder models, one to embed the query and one to embed the passage, and then compares the embedding for retrieval\n",
    "- `TfidfRetriever` in combination with a SQL or InMemory Document store for simple prototyping and debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kFwiPP60A6N7",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA:0\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
      "INFO - haystack.nodes.retriever.dense -  Init retriever using embeddings of model /home/tstad/git/ColBERT/docs/downloads/colbertv2.0\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes import EmbeddingRetriever\n",
    "\n",
    "retriever = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    embedding_model=\"/home/tstad/git/ColBERT/docs/downloads/colbertv2.0\",\n",
    "    model_format=\"colbert\",\n",
    ")\n",
    "# Important:\n",
    "# Now that we initialized the Retriever, we need to call update_embeddings() to iterate over all\n",
    "# previously indexed documents and update their embedding representation.\n",
    "# While this can be a time consuming operation (depending on the corpus size), it only needs to be done once.\n",
    "# At query time, we only need to embed the query and compare it to the existing document embeddings, which is very fast.\n",
    "# document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnVR28OXA6OA"
   },
   "source": [
    "#### Reader\n",
    "\n",
    "Similar to previous Tutorials we now initalize our reader.\n",
    "\n",
    "Here we use a FARMReader with the *deepset/roberta-base-squad2* model (see: https://huggingface.co/deepset/roberta-base-squad2)\n",
    "\n",
    "\n",
    "\n",
    "##### FARMReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fyIuWVwhA6OB"
   },
   "outputs": [],
   "source": [
    "# Load a  local model or any of the QA models on\n",
    "# Hugging Face's model hub (https://huggingface.co/models)\n",
    "\n",
    "# reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unhLD18yA6OF"
   },
   "source": [
    "### Pipeline\n",
    "\n",
    "With a Haystack `Pipeline` you can stick together your building blocks to a search pipeline.\n",
    "Under the hood, `Pipelines` are Directed Acyclic Graphs (DAGs) that you can easily customize for your own use cases.\n",
    "To speed things up, Haystack also comes with a few predefined Pipelines. One of them is the `ExtractiveQAPipeline` that combines a retriever and a reader to answer our questions.\n",
    "You can learn more about `Pipelines` in the [docs](https://haystack.deepset.ai/docs/latest/pipelinesmd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TssPQyzWA6OG"
   },
   "outputs": [],
   "source": [
    "from haystack.pipelines import DocumentSearchPipeline\n",
    "\n",
    "pipe = DocumentSearchPipeline(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXlBBxKXA6OL"
   },
   "source": [
    "## Voilà! Ask a question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Zi97Hif2A6OM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 17, 10:17:50] #> Loading codec...\n",
      "[Sep 17, 10:17:50] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Sep 17, 10:17:50] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Sep 17, 10:17:50] #> Loading IVF...\n",
      "[Sep 17, 10:17:50] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1257.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 17, 10:17:50] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 124.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# You can configure how many candidates the reader and retriever shall return\n",
    "# The higher top_k for retriever, the better (but also the slower) your answers.\n",
    "prediction = pipe.run(query=\"Who created the Dothraki vocabulary?\", params={\"Retriever\": {\"top_k\": 10}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pI0wrHylzqLa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [<Document: {'content': \"\\n==Development==\\nDavid J. Peterson, creator of the Dothraki spoken language for ''Game of Thrones''\\nThe Dothraki vocabulary was created by David J. Peterson well in advance of the adaptation. HBO hired the Language Creation Society to create the language, and after an application process involving over 30 conlangers, Peterson was chosen to develop the Dothraki language. He delivered over 1700 words to HBO before the initial shooting. Peterson drew inspiration from George R. R. Martin’s description of the language, as well as from such languages as Estonian, Inuktitut, Turkish, Russian, and Swahili.\\nDavid J. Peterson and his development of the Dothraki language were featured on an April 8, 2012 episode of CNN's ''The Next List''. He went on to create the Valyrian languages for season 3 of ''Game of Thrones''.  Peterson and his development of Dothraki were also featured on the January 8, 2017 episode of ''To Tell the Truth''.\", 'content_type': 'text', 'score': 0.5652507475670999, 'meta': {'name': '214_Dothraki_language.txt', 'vector_id': '229'}, 'embedding': None, 'id': '27baa56e5aab6b04d38f19e97e078bc6'}>,\n",
       "  <Document: {'content': \"\\n===Valyrian===\\nDavid J. Peterson, who created the Dothraki language for the first season of the show, was entrusted by the producers to design a new constructed language to depict Valyrian, the tongue of the fallen Valyrian Empire. After immersing himself in the fictional background, Peterson ended devising two languages: High Valyrian, the oldest form that was spoken at the height of the Empire and that in its purest form still exists as a language of scholarship and refinement, and the Slaver's Bay variety of Low Valyrian, a creolized version that is spoken in local dialects around the Slaver's Bay. The relationship between the two languages would be similar to the one between Classical Latin and Vulgar Latin.\\nTo translate sentences in Low Valyrian such as the ones spoken by Kraznys mo Nakloz and Missandei during the episode, Peterson would first write them into High Valyrian, and then apply a series of phonological, semantic and grammatical changes to the text.\", 'content_type': 'text', 'score': 0.5596366371182048, 'meta': {'name': '87_Valar_Dohaeris.txt', 'vector_id': '1595'}, 'embedding': None, 'id': 'b368200c210d555625bd409b0dc27be1'}>,\n",
       "  <Document: {'content': 'The \\'\\'\\'Dothraki language\\'\\'\\' is a constructed fictional language in George R. R. Martin\\'s fantasy novel series \\'\\'A Song of Ice and Fire\\'\\' and its television adaptation \\'\\'Game of Thrones\\'\\'. It is spoken by the Dothraki, a nomadic people in the series\\'s fictional world. The language was developed for the TV series by the linguist David J. Peterson, working off the Dothraki words and phrases in Martin\\'s novels.\\n, the language comprised 3163 words, not all of which have been made public. In 2012, 146 newborn girls in the United States were named \"Khaleesi\", the Dothraki term for the wife of a \\'\\'khal\\'\\' or ruler, and the title adopted in the series by Daenerys Targaryen. Dothraki and Valyrian have been described as \"the most convincing fictional tongues since Elvish\".', 'content_type': 'text', 'score': 0.5590589547516776, 'meta': {'name': '214_Dothraki_language.txt', 'vector_id': '306'}, 'embedding': None, 'id': '308dca876f94e5e839187f1463693015'}>,\n",
       "  <Document: {'content': '\\n===Creation===\\nDavid J. Peterson, creator of the spoken Valyrian languages for \\'\\'Game of Thrones\\'\\'\\nTo create the Dothraki and Valyrian languages to be spoken in \\'\\'Game of Thrones\\'\\', HBO selected the linguist David J. Peterson through a competition among conlangers. The producers gave Peterson a largely free hand in developing the languages, as, according to Peterson, George R. R. Martin himself was not very interested in the linguistic aspect of his works. The already published novels include only a few words of High Valyrian, including \\'\\'valar morghulis\\'\\' (\"all men must die\"), \\'\\'valar dohaeris\\'\\' (\"all men must serve\") and \\'\\'dracarys\\'\\' (\"dragonfire\"). For the forthcoming novel \\'\\'The Winds of Winter\\'\\', Peterson has supplied Martin with additional Valyrian translations.\\nPeterson commented that he considered Martin\\'s choice of \\'\\'dracarys\\'\\' unfortunate because of its (presumably intended) similarity to the Latin word for dragon, \\'\\'\\'\\'. Because the Latin language does not exist in the fictional world of \\'\\'A Song of Ice and Fire\\'\\', Peterson chose to treat the similarity as coincidental and made \\'\\'dracarys\\'\\' an independent lexeme; his High Valyrian term for dragon is \\'\\'zaldrīzes\\'\\'. The phrases \\'\\'valar morghulis\\'\\' and \\'\\'valar dohaeris\\'\\', on the other hand, became the foundation of the language\\'s conjugation system. Another word, \\'\\'trēsy\\'\\', meaning \"son\", was coined in honour of Peterson\\'s 3000th Twitter follower.\\nPeterson did not create a High Valyrian writing system at the time, but he commented that he \"was thinking something more like Egyptian\\'s system of hieroglyphs—not in style, necessarily, but in their functionality. Egyptians had an alphabet, of sorts, a couple of phonetically-based systems, and a logography all layered on top of one another.\" In the third season\\'s episode \"The Bear and the Maiden Fair\", Talisa is seen writing a Valyrian letter in the Latin alphabet, because according to Peterson, \"it didn\\'t seem worthwhile to create an entire writing system for what ultimately is kind of a throwaway shot\".\\nAt the start of June 2013, there were 667 High Valyrian words.', 'content_type': 'text', 'score': 0.5566309599222533, 'meta': {'name': '213_Valyrian_languages.txt', 'vector_id': '891'}, 'embedding': None, 'id': '6d8f997d8c71f866f19b85222e8a740'}>,\n",
       "  <Document: {'content': 'The \\'\\'\\'Valyrian languages\\'\\'\\' are a fictional language family in the \\'\\'A Song of Ice and Fire\\'\\' series of fantasy novels by George R. R. Martin, and in their television adaptation \\'\\'Game of Thrones\\'\\'.\\nIn the novels, High Valyrian and its descendant languages are often mentioned but not developed beyond a few words. For the TV series, linguist David J. Peterson created the High Valyrian language, as well as the derivative languages Astapori and Meereenese Valyrian, based on the fragments given in the novels. Valyrian and Dothraki have been described as \"the most convincing fictional tongues since Elvish\".', 'content_type': 'text', 'score': 0.551071269307342, 'meta': {'name': '213_Valyrian_languages.txt', 'vector_id': '1474'}, 'embedding': None, 'id': 'a552cd7ba07635fd475c9aaaa026c9f0'}>,\n",
       "  <Document: {'content': '\\n===Language constraints===\\nThe Dothraki language was developed under two significant constraints.  First, the language had to match the uses already put down in the books. Secondly, it had to be easily pronounceable or learnable by the actors. These two constraints influenced the grammar and phonology of the language: for instance, as in English, there is no contrast between aspirated and unaspirated stops.', 'content_type': 'text', 'score': 0.5492924959955306, 'meta': {'name': '214_Dothraki_language.txt', 'vector_id': '1172'}, 'embedding': None, 'id': '8767e85c7a9bcec61f95e13bb61f3e98'}>,\n",
       "  <Document: {'content': \"\\n==External links==\\n*  www.dothraki.com—A site managed by David J. Peterson, where he blogs about the languages he's constructed for ''Game of Thrones''\\n*  Tongues of Ice and Fire wiki—fan wiki collating information about the constructed languages in ''Game of Thrones''\\n**  Learning High Valyrian on the Tongues of Ice and Fire wiki\\n**  Learning Astapori Valyrian on the Tongues of Ice and Fire wiki\\n*  Tongues of Ice and Fire forums, including Valyrian-specific discussion threads (for  beginners and about  language updates).\", 'content_type': 'text', 'score': 0.5484413298757518, 'meta': {'name': '213_Valyrian_languages.txt', 'vector_id': '1021'}, 'embedding': None, 'id': '788b0a68eb75c5f5969d3414816204b4'}>,\n",
       "  <Document: {'content': '\\n==Phonology and romanization==\\nDavid Peterson has said, \"You know, most people probably don\\'t really know what Arabic actually sounds like, so to an untrained ear, it might sound like Arabic. To someone who knows Arabic, it doesn\\'t. I tend to think of the sound as a mix between Arabic (minus the distinctive pharyngeals) and Spanish, due to the dental consonants.\"\\nRegarding the orthography, the Dothraki themselves do not have a writing system—nor do many of the surrounding peoples (e.g., the Lhazareen). If there were to be any written examples of Dothraki in the \\'\\'A Song of Ice and Fire\\'\\' universe, it would be in a writing system developed in the Free Cities and adapted to Dothraki, or in some place like Ghis or Qarth, which do have writing systems.', 'content_type': 'text', 'score': 0.5458861500024496, 'meta': {'name': '214_Dothraki_language.txt', 'vector_id': '846'}, 'embedding': None, 'id': '694a3bab540fc5a5033b95236c3c323e'}>,\n",
       "  <Document: {'content': '\\n====Dothraki Sea ====\\nThe Dothraki Sea is a vast, flat grassland on Essos. It is inhabited by the Dothraki people, a copper-skinned race of warlike nomads with their own language and unique culture. The Dothraki live in hordes called khalasars, each led by a chief called a khal. Khalasars are broken into groups, called khas, which are each led by one of the khal\\'s captains, called kos. Each khal and his khalasar owe fealty to a ruling council of royal priestesses, called the dosh khaleen, whose members are each a former khal\\'s consort, called a khaleesi during the reign of her husband, one who became part of the dosh khaleen following his death.\\nDothraki are expert riders and their horses are of prime importance in their culture, used for food, transportation, raw materials, warfare, and establishing social standing. They regularly raid other peoples.\\nGeorge R. R. Martin said \"The Dothraki were actually fashioned as an amalgam of a number of steppe and plains cultures ... Mongols and Huns, certainly, but also Alans, Sioux, Cheyenne, and various other Amerindian tribes ... seasoned with a dash of pure fantasy. So any resemblance to Arabs or Turks is coincidental. Well, except to the extent that the Turkic peoples|historic Turks were also originally horsemen of the steppes, not unlike the Alans, Huns, and the rest.\" However, he also noted that \"In general, though, while I do draw inspiration from history, I try to avoid direct one-for-one transplants, so it would not be correct to say that the Dothraki are Mongols.\"\\nThe Dothraki have only one permanent city, called Vaes Dothrak, which serves as their capital. The dosh khaleen hold the city as their seat. It is filled with statues stolen from other cities the Dothraki conquered or raided. There is a law that no Dothraki may shed blood within the boundaries of Vaes Dothrak and that those who do are cursed. Two gigantic bronze stallions, whose hooves meet midair, form an arch above the entryway to the city. For the first season of the TV adaptation, Sandy Brae in the Mourne Mountains of northern Ireland was chosen to stand in for Vaes Dothrak; the bronze stallions making up the Horse Gate as the main entrance of Vaes Dothrak were later C.G.I.ed on two pedestals erected on location.', 'content_type': 'text', 'score': 0.5447626465466445, 'meta': {'name': '195_World_of_A_Song_of_Ice_and_Fire.txt', 'vector_id': '657'}, 'embedding': None, 'id': '553bbd758a0fb58d160ccae5815daa7e'}>,\n",
       "  <Document: {'content': '\\n==External links==\\n*  The official Dothraki blog at dothraki.com\\n*  The Dothraki Language Wiki at wiki.dothraki.org\\n*  The LCS blog about the language at dothraki.conlang.org', 'content_type': 'text', 'score': 0.5446463958413529, 'meta': {'name': '214_Dothraki_language.txt', 'vector_id': '2021'}, 'embedding': None, 'id': 'de57a45af9f92f4c62e91c6649641de2'}>],\n",
       " 'root_node': 'Query',\n",
       " 'params': {'Retriever': {'top_k': 10}},\n",
       " 'query': 'Who created the Dothraki vocabulary?',\n",
       " 'node_id': 'Retriever'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "kXE84-2_zqLa"
   },
   "source": [
    "## About us\n",
    "\n",
    "This [Haystack](https://github.com/deepset-ai/haystack/) notebook was made with love by [deepset](https://deepset.ai/) in Berlin, Germany\n",
    "\n",
    "We bring NLP to the industry via open source!\n",
    "  \n",
    "Our focus: Industry specific language models & large scale QA systems.  \n",
    "  \n",
    "Some of our other work: \n",
    "- [German BERT](https://deepset.ai/german-bert)\n",
    "- [GermanQuAD and GermanDPR](https://deepset.ai/germanquad)\n",
    "- [FARM](https://github.com/deepset-ai/FARM)\n",
    "\n",
    "Get in touch:\n",
    "[Twitter](https://twitter.com/deepset_ai) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Slack](https://haystack.deepset.ai/community/join) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://deepset.ai)\n",
    "\n",
    "By the way: [we're hiring!](https://www.deepset.ai/jobs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tutorial6_Better_Retrieval_via_Embedding_Retrieval.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.7.11 ('haystack-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1c4180befe5334d9af26d84758dc08f43161c3b98a4eb4d4a43d7491d015a65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
